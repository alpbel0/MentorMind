# MentorMind - AI Evaluator Training System

**Versiyon:** 1.0.0-MVP  
**Durum:** ğŸš§ Aktif GeliÅŸtirme  
**Son GÃ¼ncelleme:** 26 Ocak 2025

---

## ğŸ“‹ Ä°Ã§indekiler

- [Proje HakkÄ±nda](#-proje-hakkÄ±nda)
- [Ã–zellikler](#-Ã¶zellikler)
- [Mimari Genel BakÄ±ÅŸ](#ï¸-mimari-genel-bakÄ±ÅŸ)
- [Teknoloji Stack](#-teknoloji-stack)
- [Kurulum](#-kurulum)
- [VeritabanÄ± YapÄ±sÄ±](#ï¸-veritabanÄ±-yapÄ±sÄ±)
- [Workflow - Sistem AkÄ±ÅŸÄ±](#-workflow---sistem-akÄ±ÅŸÄ±)
- [ChromaDB YapÄ±sÄ±](#-chromadb-yapÄ±sÄ±)
- [Logging Sistemi](#-logging-sistemi)
- [API Endpoints](#-api-endpoints)
- [Seed Data](#-seed-data)
- [KullanÄ±m Ã–rnekleri](#-kullanÄ±m-Ã¶rnekleri)
- [Development Guide](#-development-guide)
- [Troubleshooting](#-troubleshooting)

---

## ğŸ¯ Proje HakkÄ±nda

**MentorMind**, EvalOps (AI deÄŸerlendirme operasyonlarÄ±) alanÄ±nda kendini geliÅŸtirmek isteyenler iÃ§in tasarlanmÄ±ÅŸ bir kiÅŸisel eÄŸitim platformudur. GPT-4o mentor modeli aracÄ±lÄ±ÄŸÄ±yla AI model cevaplarÄ±nÄ± deÄŸerlendirme becerisini 8 farklÄ± metrik Ã¼zerinden geliÅŸtirmenizi saÄŸlar.

### ğŸ“ EÄŸitim Felsefesi

**Metrik-OdaklÄ± Ã–ÄŸrenme:**
- KullanÄ±cÄ± bir metrik seÃ§er (Ã¶rn: "Truthfulness")
- Sistem o metriÄŸi test eden sorular Ã¼retir
- KullanÄ±cÄ± 8 metrik iÃ§in de deÄŸerlendirme yapar (bias'Ä± Ã¶nler)
- GPT-4o mentor objektif feedback verir
- GeÃ§miÅŸ hatalar ChromaDB'de saklanÄ±r ve hatÄ±rlatÄ±lÄ±r

### ğŸ“Š 8 DeÄŸerlendirme MetriÄŸi

1. **Truthfulness** - GerÃ§eklik/DoÄŸruluk
2. **Helpfulness** - YardÄ±mcÄ± Olma
3. **Safety** - GÃ¼venlik
4. **Bias** - Ã–nyargÄ±
5. **Clarity** - AÃ§Ä±klÄ±k
6. **Consistency** - TutarlÄ±lÄ±k
7. **Efficiency** - Verimlilik
8. **Robustness** - DayanÄ±klÄ±lÄ±k

---

## âœ¨ Ã–zellikler

### ğŸ¯ Metrik-OdaklÄ± Ã‡alÄ±ÅŸma
- Belirli bir metrikte odaklanarak Ã§alÄ±ÅŸ
- Her deÄŸerlendirmede 8 metrik iÃ§in yorum yap (objektif kalabilmek iÃ§in)
- Primary metrik %70 aÄŸÄ±rlÄ±klÄ± deÄŸerlendirilir

### ğŸŠ Soru Havuzu Sistemi
- **Yeni Ãœret:** Claude Haiku 4.5 yeni soru Ã¼retir
- **Havuzdan SeÃ§:** Mevcut sorulardan rastgele seÃ§
- AynÄ± soru farklÄ± modeller tarafÄ±ndan cevaplanabilir
- Model performans karÅŸÄ±laÅŸtÄ±rmasÄ± yapabilirsin

### ğŸ¤– Ã‡oklu Model DesteÄŸi
- GPT-3.5-turbo
- GPT-4o-mini
- Claude 3.5 Haiku
- Gemini 2.0 Flash
- (Dilersen daha fazla eklenebilir)

### âš–ï¸ Ä°ki AÅŸamalÄ± Judge Sistemi
**Stage 1 - Independent Evaluation:**
- GPT-4o senin puanÄ±nÄ± gÃ¶rmeden deÄŸerlendirir
- Objektif, baÄŸÄ±msÄ±z deÄŸerlendirme

**Stage 2 - Mentoring:**
- Senin puanÄ±nla kÄ±yaslar
- GeÃ§miÅŸ hatalarÄ±nÄ± ChromaDB'den Ã§eker
- Alignment gap analizi yapar
- Meta-puan verir (1-5)
- YapÄ±cÄ± feedback verir

### ğŸ§  ChromaDB HafÄ±za
- Her deÄŸerlendirme sonrasÄ± hafÄ±zaya eklenir
- GeÃ§miÅŸ hatalar judge'a hatÄ±rlatÄ±lÄ±r
- "Truthfulness'ta 3. kez aynÄ± hatayÄ± yapÄ±yorsun" gibi feedback'ler

---

## ğŸ—ï¸ Mimari Genel BakÄ±ÅŸ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         KULLANICI (SEN)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Metrik SeÃ§imi                                               â”‚
â”‚     - Metrik: "Truthfulness", "Safety", "Clarity", etc.        â”‚
â”‚     - Soru KaynaÄŸÄ±: "Yeni Ãœret" veya "Havuzdan SeÃ§"           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Soru HazÄ±rlama                                              â”‚
â”‚     EÄER "Yeni Ãœret":                                           â”‚
â”‚       - question_prompts'tan primary_metric="Truthfulness" seÃ§ â”‚
â”‚       - Claude Haiku 4.5'e gÃ¶nder                             â”‚
â”‚       - Soru Ã¼ret ve questions'a kaydet                        â”‚
â”‚     EÄER "Havuzdan SeÃ§":                                        â”‚
â”‚       - questions'tan primary_metric="Truthfulness" seÃ§        â”‚
â”‚       - En az kullanÄ±lmÄ±ÅŸ soruyu getir                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. K Model Cevaplama                                           â”‚
â”‚     - Rastgele model seÃ§ (bu soruyu cevaplamamÄ±ÅŸ olanlardan)  â”‚
â”‚     - Soruyu modele gÃ¶nder                                     â”‚
â”‚     - model_responses'a kaydet                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. KULLANICI DEÄERLENDÄ°RMESÄ°                                   â”‚
â”‚     - GÃ¶rÃ¼r: Soru + K Model CevabÄ±                             â”‚
â”‚     - Yapar: HER 8 METRÄ°K iÃ§in puan (1-5 veya "-") + aÃ§Ä±klama â”‚
â”‚     - Primary/bonus bilgisi GÄ°ZLÄ° (bias Ã¶nlenmesi iÃ§in)       â”‚
â”‚     - user_evaluations'a kaydet                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. JUDGE EVALUATION - STAGE 1 (Independent)                   â”‚
â”‚     - GPT-4o senin puanÄ±nÄ± GÃ–RMEDÄ°                             â”‚
â”‚     - Sadece: Soru + Model CevabÄ± + Reference                  â”‚
â”‚     - Her 8 metrik iÃ§in kendi puanÄ±nÄ± verir                    â”‚
â”‚     - independent_scores'a kaydedilir                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. ChromaDB QUERY (GeÃ§miÅŸ Hatalar)                            â”‚
â”‚     - Query: "User evaluating Truthfulness in Math category"   â”‚
â”‚     - Son 5 benzer deÄŸerlendirme getir                         â”‚
â”‚     - Mistake patterns Ã§Ä±kar                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. JUDGE EVALUATION - STAGE 2 (Mentoring)                     â”‚
â”‚     - GPT-4o senin puanÄ±nÄ± GÃ–RÃœR                               â”‚
â”‚     - Kendi puanÄ±yla kÄ±yaslar (alignment gap)                  â”‚
â”‚     - GeÃ§miÅŸ hatalarÄ±nÄ± hatÄ±rlatÄ±r                             â”‚
â”‚     - Primary metric %70 aÄŸÄ±rlÄ±klÄ±                             â”‚
â”‚     - Meta-puan verir (1-5)                                    â”‚
â”‚     - YapÄ±cÄ± feedback: improvement + positive                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. KAYIT ve HAFIZA GÃœNCELLEMESÄ°                                â”‚
â”‚     - judge_evaluations'a kaydet                               â”‚
â”‚     - ChromaDB'ye ekle (future memory)                         â”‚
â”‚     - KullanÄ±cÄ±ya feedback gÃ¶ster                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Teknoloji Stack

### Backend
- **Python 3.11+** - Ana dil
- **FastAPI** - REST API framework
- **PostgreSQL** - Ä°liÅŸkisel veritabanÄ±
- **ChromaDB** - Vector database (embedding hafÄ±zasÄ±)
- **Anthropic Claude API** - Soru Ã¼retimi (Haiku 4.5)
- **OpenAI API** - GPT-4o judge + K modeller + embeddings
- **Google Gemini API** - K model (Gemini 2.0 Flash)
- **Docker & Docker Compose** - Konteynerizasyon

### LLM Modelleri
**Soru Ãœretimi:**
- Claude Haiku 4.5 (`claude-haiku-4-5-20251001`) - HÄ±zlÄ± ve maliyet-etkin

**K Models (DeÄŸerlendirilecek):**
- `gpt-3.5-turbo`
- `gpt-4o-mini`
- `claude-3-5-haiku-20241022`
- `gemini-2.0-flash-exp`

**Judge Model:**
- `gpt-4o` (GPT-4o latest)

**Embeddings:**
- `text-embedding-3-small` (OpenAI)

### Dependencies (Ana)
```txt
fastapi==0.109.0
uvicorn==0.27.0
anthropic==0.18.1
openai==1.12.0
google-generativeai==0.3.2
chromadb==0.4.22
sqlalchemy==2.0.25
psycopg2-binary==2.9.9
pydantic==2.5.3
python-dotenv==1.0.0
```

---

## ğŸš€ Kurulum

### 1. Projeyi Klonla
```bash
git clone https://github.com/yourusername/mentormind.git
cd mentormind
```

### 2. Environment AyarlarÄ±
```bash
cp .env.example .env
```

`.env` dosyasÄ±nÄ± dÃ¼zenle:
```env
# API Keys
ANTHROPIC_API_KEY=sk-ant-xxxxx
OPENAI_API_KEY=sk-xxxxx
GOOGLE_API_KEY=xxxxx

# Database (PostgreSQL)
DATABASE_URL=postgresql://mentormind:mentormind_password@postgres:5432/mentormind
POSTGRES_USER=mentormind
POSTGRES_PASSWORD=mentormind_password
POSTGRES_DB=mentormind

# ChromaDB
CHROMA_HOST=chromadb
CHROMA_PORT=8000
CHROMA_PERSIST_DIR=/chroma_data
CHROMA_COLLECTION_NAME=evaluation_memory

# Logging
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# App Settings
ENVIRONMENT=development
```

### 3. Docker ile Ã‡alÄ±ÅŸtÄ±r
```bash
# TÃ¼m servisleri baÅŸlat
docker-compose up -d --build

# LoglarÄ± takip et
docker-compose logs -f

# Durdur
docker-compose down

# Database + volume'larÄ± sil (factory reset)
docker-compose down -v
```

### 4. Database Ä°lklendir
```bash
# Ä°lk kez Ã§alÄ±ÅŸtÄ±rÄ±yorsan
docker-compose exec backend python scripts/init_db.py

# Seed data ekle (question_prompts)
docker-compose exec backend python scripts/seed_data.py
```

### 5. Health Check
```bash
# Backend health
curl http://localhost:8000/health

# ChromaDB health
curl http://localhost:8001/api/v1/heartbeat

# PostgreSQL health
docker-compose exec postgres pg_isready -U mentormind
```

---

## ğŸ—„ï¸ VeritabanÄ± YapÄ±sÄ±

### ERD (Entity Relationship Diagram)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   question_prompts      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)                 â”‚
â”‚ primary_metric          â”‚  "Truthfulness"
â”‚ bonus_metrics (JSON)    â”‚  ["Clarity", "Helpfulness"]
â”‚ question_type           â”‚  "hallucination_test"
â”‚ user_prompt             â”‚  Claude'a gÃ¶nderilecek prompt
â”‚ golden_examples (JSON)  â”‚  Ã–rnek soru-cevaplar
â”‚ difficulty              â”‚  "easy", "medium", "hard"
â”‚ category_hint           â”‚  "prefer_medical", "any"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (Prompt seÃ§ilir, Claude soru Ã¼retir)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      questions          â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)                 â”‚  "q_20250126_143052_abc123"
â”‚ question                â”‚  Soru metni
â”‚ category                â”‚  "Math", "Coding", "Medical", "General"
â”‚ reference_answer        â”‚  Ä°deal cevap
â”‚ expected_behavior       â”‚  Beklenen davranÄ±ÅŸ
â”‚ rubric_breakdown (JSON) â”‚  {"1": "desc", "2": "desc", ...}
â”‚ primary_metric          â”‚  "Truthfulness"
â”‚ bonus_metrics (JSON)    â”‚  ["Clarity", "Helpfulness"]
â”‚ question_prompt_id (FK) â”‚
â”‚ question_type           â”‚
â”‚ difficulty              â”‚
â”‚ category_hint_used      â”‚
â”‚ times_used              â”‚  Soru havuzu iÃ§in
â”‚ first_used_at           â”‚
â”‚ last_used_at            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (K Model'e sorulur)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   model_responses       â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)                 â”‚  "resp_20250126_143052_xyz789"
â”‚ question_id (FK)        â”‚
â”‚ model_name              â”‚  "gpt-3.5-turbo"
â”‚ response_text           â”‚  Model'in cevabÄ±
â”‚ evaluated               â”‚  TRUE/FALSE
â”‚ evaluation_id           â”‚
â”‚ UNIQUE(question, model) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (KullanÄ±cÄ± deÄŸerlendirir)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   user_evaluations      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)                 â”‚  "eval_20250126_143000_aaa111"
â”‚ response_id (FK)        â”‚
â”‚ evaluations (JSON)      â”‚  Her 8 metrik iÃ§in {"score": 1-5, "reasoning": "..."}
â”‚ judged                  â”‚  TRUE/FALSE
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (GPT-4o deÄŸerlendirir)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   judge_evaluations     â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ id (PK)                 â”‚  "judge_20250126_143100_bbb222"
â”‚ user_evaluation_id (FK) â”‚
â”‚ independent_scores (JSON)â”‚  Stage 1 - BaÄŸÄ±msÄ±z skorlar
â”‚ alignment_analysis (JSON)â”‚  Stage 2 - Gap analizi
â”‚ judge_meta_score (1-5)  â”‚  Senin deÄŸerlendirme kalitene puan
â”‚ overall_feedback        â”‚  Genel mentÃ¶rlÃ¼k yorumu
â”‚ improvement_areas (JSON)â”‚  ["area1", "area2"]
â”‚ positive_feedback (JSON)â”‚  ["good1", "good2"]
â”‚ vector_context (JSON)   â”‚  ChromaDB'den gelen geÃ§miÅŸ hatalar
â”‚ primary_metric          â”‚  "Truthfulness"
â”‚ primary_metric_gap      â”‚  1.2
â”‚ weighted_gap            â”‚  0.8
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Database Schema Files

```sql
-- backend/schemas/01_question_prompts.sql
CREATE TABLE question_prompts (
    id SERIAL PRIMARY KEY,
    
    -- Metrics (user only selects primary, bonus is hidden)
    primary_metric TEXT NOT NULL,
    bonus_metrics JSON NOT NULL,
    
    -- Question generation
    question_type TEXT NOT NULL,
    user_prompt TEXT NOT NULL,
    golden_examples JSON,
    
    -- Metadata
    difficulty TEXT CHECK(difficulty IN ('easy', 'medium', 'hard')) DEFAULT 'medium',
    category_hint TEXT DEFAULT 'any',
    
    -- Timestamps
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- Constraints
    UNIQUE(primary_metric, question_type)
);

CREATE INDEX idx_primary_metric ON question_prompts(primary_metric);
CREATE INDEX idx_difficulty ON question_prompts(difficulty);
```

```sql
-- backend/schemas/02_questions.sql
CREATE TABLE questions (
    id TEXT PRIMARY KEY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- Question content
    question TEXT NOT NULL,
    category TEXT NOT NULL,
    reference_answer TEXT,
    expected_behavior TEXT,
    rubric_breakdown JSON NOT NULL,
    
    -- DENORMALIZED from question_prompts (for performance)
    primary_metric TEXT NOT NULL,
    bonus_metrics JSON NOT NULL,
    question_type TEXT NOT NULL,
    difficulty TEXT NOT NULL,
    category_hint_used TEXT,
    
    -- Reference
    question_prompt_id INTEGER NOT NULL,
    
    -- Usage tracking
    times_used INTEGER DEFAULT 0,
    first_used_at TIMESTAMP,
    last_used_at TIMESTAMP,
    
    FOREIGN KEY (question_prompt_id) REFERENCES question_prompts(id)
);

CREATE INDEX idx_questions_primary_metric ON questions(primary_metric);
CREATE INDEX idx_questions_category ON questions(category);
CREATE INDEX idx_questions_times_used ON questions(times_used);
CREATE INDEX idx_questions_difficulty ON questions(difficulty);
```

```sql
-- backend/schemas/03_model_responses.sql
CREATE TABLE model_responses (
    id TEXT PRIMARY KEY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    question_id TEXT NOT NULL,
    
    model_name TEXT NOT NULL,
    response_text TEXT NOT NULL,
    
    evaluated BOOLEAN DEFAULT FALSE,
    evaluation_id TEXT,
    
    FOREIGN KEY (question_id) REFERENCES questions(id),
    UNIQUE(question_id, model_name)
);

CREATE INDEX idx_model_responses_question ON model_responses(question_id);
CREATE INDEX idx_model_responses_model ON model_responses(model_name);
CREATE INDEX idx_model_responses_evaluated ON model_responses(evaluated);
```

```sql
-- backend/schemas/04_user_evaluations.sql
CREATE TABLE user_evaluations (
    id TEXT PRIMARY KEY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    response_id TEXT NOT NULL,

    evaluations JSON NOT NULL,

    judged BOOLEAN DEFAULT FALSE,

    FOREIGN KEY (response_id) REFERENCES model_responses(id)
);

CREATE INDEX idx_user_evaluations_response ON user_evaluations(response_id);
CREATE INDEX idx_user_evaluations_judged ON user_evaluations(judged);
CREATE INDEX idx_user_evaluations_created ON user_evaluations(created_at DESC);
```

```sql
-- backend/schemas/05_judge_evaluations.sql
CREATE TABLE judge_evaluations (
    id TEXT PRIMARY KEY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    user_evaluation_id TEXT NOT NULL,
    
    independent_scores JSON NOT NULL,
    
    alignment_analysis JSON NOT NULL,
    judge_meta_score INTEGER CHECK(judge_meta_score BETWEEN 1 AND 5),
    overall_feedback TEXT,
    improvement_areas JSON,
    positive_feedback JSON,
    
    vector_context JSON,
    
    primary_metric TEXT NOT NULL,
    primary_metric_gap REAL,
    weighted_gap REAL,
    
    FOREIGN KEY (user_evaluation_id) REFERENCES user_evaluations(id)
);

CREATE INDEX idx_judge_evaluations_user_eval ON judge_evaluations(user_evaluation_id);
CREATE INDEX idx_judge_evaluations_meta_score ON judge_evaluations(judge_meta_score);
CREATE INDEX idx_judge_evaluations_primary_metric ON judge_evaluations(primary_metric);
CREATE INDEX idx_judge_evaluations_created ON judge_evaluations(created_at DESC);
```

---

## ğŸ”„ Workflow - Sistem AkÄ±ÅŸÄ±

### AkÄ±ÅŸ 1: Yeni Soru Ãœretme + DeÄŸerlendirme

```python
# 1. KullanÄ±cÄ± metrik seÃ§er
POST /api/evaluations/start
{
  "primary_metric": "Truthfulness",
  "use_pool": false  # Yeni soru Ã¼ret
}

# Backend:
# - question_prompts'tan primary_metric="Truthfulness" random seÃ§
# - category_hint'e gÃ¶re kategori belirle (backend)
# - Claude Haiku 4.5'e gÃ¶nder
# - questions'a kaydet
# - HenÃ¼z cevaplamamÄ±ÅŸ K model seÃ§
# - Model'e gÃ¶nder
# - model_responses'a kaydet

# Response:
{
  "question_id": "q_20250126_143052_abc123",
  "response_id": "resp_20250126_143052_xyz789",
  "question": "2024 Nobel Kimya Ã–dÃ¼lÃ¼'nÃ¼ kim kazandÄ±?",
  "model_response": "Jennifer Doudna kazandÄ±...",
  "model_name": "gpt-3.5-turbo",
  "category": "General",
  "primary_metric": "Truthfulness",  # HIDDEN from user
  "bonus_metrics": ["Clarity", "Helpfulness"]  # HIDDEN
}

# 2. KullanÄ±cÄ± deÄŸerlendirir
POST /api/evaluations/submit
{
  "response_id": "resp_20250126_143052_xyz789",
  "evaluations": {
    "Truthfulness": {"score": 2, "reasoning": "YanlÄ±ÅŸ bilgi, bÃ¶yle bir ÅŸey olmadÄ±"},
    "Helpfulness": {"score": 1, "reasoning": "YanlÄ±ÅŸ bilgi zararlÄ±"},
    "Safety": {"score": null, "reasoning": "GeÃ§erli deÄŸil"},
    "Bias": {"score": null, "reasoning": "GeÃ§erli deÄŸil"},
    "Clarity": {"score": 5, "reasoning": "Net cevap ama yanlÄ±ÅŸ"},
    "Consistency": {"score": null, "reasoning": "Tek cevap"},
    "Efficiency": {"score": 4, "reasoning": "KÄ±sa ve Ã¶z"},
    "Robustness": {"score": 1, "reasoning": "Trap question'da baÅŸarÄ±sÄ±z"}
  }
}

# Backend:
# - user_evaluations'a kaydet
# - Asenkron judge task baÅŸlat
# - Hemen response dÃ¶n (kullanÄ±cÄ± beklemez)

# Response:
{
  "evaluation_id": "eval_20250126_143000_aaa111",
  "status": "submitted",
  "judge_status": "processing"
}

# 3. Judge (Arka planda asenkron)
# - Stage 1: Independent evaluation
# - ChromaDB query (geÃ§miÅŸ hatalar)
# - Stage 2: Mentoring
# - judge_evaluations'a kaydet
# - ChromaDB'ye ekle (hafÄ±za)
# - user_evaluations.judged = TRUE

# 4. KullanÄ±cÄ± feedback'i Ã§eker
GET /api/evaluations/eval_20250126_143000_aaa111/feedback

# Response:
{
  "evaluation_id": "eval_20250126_143000_aaa111",
  "judge_meta_score": 5,
  "overall_feedback": "MÃ¼kemmel deÄŸerlendirme! Truthfulness'ta Ã§ok objektif davrandÄ±n...",
  "alignment_analysis": {
    "Truthfulness": {
      "user_score": 2,
      "judge_score": 2,
      "gap": 0,
      "verdict": "aligned",
      "feedback": "DoÄŸru tespit, model tamamen uyduruyor"
    },
    ...
  },
  "improvement_areas": [],
  "positive_feedback": [
    "Truthfulness'ta mÃ¼kemmel tespit",
    "Clarity skorunda objektif kaldÄ±n"
  ],
  "past_patterns_referenced": ["over_penalizing_minor_errors"]
}
```

### AkÄ±ÅŸ 2: Havuzdan Soru SeÃ§me

```python
# 1. KullanÄ±cÄ± havuzdan seÃ§er
POST /api/evaluations/start
{
  "primary_metric": "Safety",
  "use_pool": true  # Havuzdan seÃ§
}

# Backend:
# - questions'ta primary_metric="Safety" + times_used en az olan seÃ§
# - Bu soruyu hangi modeller cevapladÄ±?
# - HenÃ¼z cevaplamamÄ±ÅŸ model seÃ§
# - Model'e gÃ¶nder
# - model_responses'a kaydet
# - questions.times_used++

# Rest aynÄ±...
```

---

## ğŸ§  ChromaDB YapÄ±sÄ±

### Collection KonfigÃ¼rasyonu

```python
import chromadb
from chromadb.config import Settings

# Client
client = chromadb.Client(Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory="/chroma_data"
))

# Collection
collection = client.get_or_create_collection(
    name="evaluation_memory",
    metadata={
        "description": "User evaluation patterns and past mistakes",
        "hnsw:space": "cosine"  # Similarity metric
    },
    embedding_function=OpenAIEmbeddingFunction(
        api_key=OPENAI_API_KEY,
        model_name="text-embedding-3-small"
    )
)
```

### Document Format

```python
# Her judge evaluation sonrasÄ± ChromaDB'ye eklenir
{
    "document": """
User Evaluation ID: eval_20250126_143000_aaa111
Category: Math
Primary Metric: Truthfulness
User Scores: {"Truthfulness": 4, "Clarity": 5, ...}
Judge Scores: {"Truthfulness": 3, "Clarity": 5, ...}
Judge Meta Score: 3/5
Primary Gap: 1.0
Feedback: Ã‡ok yumuÅŸak deÄŸerlendirdin. Model'in detay eksikliÄŸi daha kritikti...
    """,
    
    "metadata": {
        "evaluation_id": "eval_20250126_143000_aaa111",
        "judge_id": "judge_20250126_143100_bbb222",
        "category": "Math",
        "primary_metric": "Truthfulness",
        "judge_meta_score": 3,
        "alignment_gap": 1.0,
        "mistake_pattern": "over_estimating_minor_errors",
        "timestamp": "2025-01-26T14:31:00Z"
    },
    
    "id": "eval_20250126_143000_aaa111"
}
```

### Query (Judge tarafÄ±ndan)

```python
def get_past_mistakes(primary_metric, category, n_results=5):
    query_text = f"User evaluating {primary_metric} in {category} category"
    
    results = collection.query(
        query_texts=[query_text],
        n_results=n_results,
        where={
            "$and": [
                {"primary_metric": primary_metric},
                {"category": category}
            ]
        }
    )
    
    return results
```

**Example Query Result:**
```json
{
  "ids": [["eval_001", "eval_042", "eval_089"]],
  "documents": [["User Evaluation...", "...", "..."]],
  "metadatas": [[
    {
      "evaluation_id": "eval_001",
      "primary_metric": "Truthfulness",
      "alignment_gap": 1.2,
      "mistake_pattern": "over_estimating_minor_errors",
      "timestamp": "2025-01-20T10:30:00Z"
    },
    {...},
    {...}
  ]],
  "distances": [[0.12, 0.18, 0.25]]
}
```

---

## ğŸ“ Logging Sistemi

### Log YapÄ±sÄ±

MentorMind kapsamlÄ± bir logging sistemi kullanÄ±r:

```
data/
â””â”€â”€ logs/
    â”œâ”€â”€ mentormind.log         # TÃ¼m loglar (DEBUG+)
    â”œâ”€â”€ mentormind.log.1       # Rotated backup
    â”œâ”€â”€ mentormind.log.2
    â”œâ”€â”€ mentormind.log.3
    â”œâ”€â”€ mentormind.log.4
    â”œâ”€â”€ mentormind.log.5
    â”œâ”€â”€ errors.log             # Sadece hatalar (ERROR+)
    â”œâ”€â”€ errors.log.1
    â””â”€â”€ llm_calls.jsonl        # LLM API Ã§aÄŸrÄ±larÄ± (JSON Lines)
```

### Log Seviyeleri

- **DEBUG:** DetaylÄ± debugging bilgileri
- **INFO:** Normal operasyonlar (API requests, workflow steps)
- **WARNING:** Potansiyel sorunlar
- **ERROR:** Hatalar
- **CRITICAL:** Sistem seviyesi baÅŸarÄ±sÄ±zlÄ±klar

### Log FormatÄ±

**Console (stdout):**
```
2025-01-26 14:30:52 - mentormind.api - INFO - â†’ POST /api/evaluations/start
```

**File (detailed):**
```
2025-01-26 14:30:52 - mentormind.judge_service - DEBUG - [judge_service.py:45] - Starting Stage 1 evaluation for eval_001
```

### LLM API Call Tracking

Her LLM API Ã§aÄŸrÄ±sÄ± `llm_calls.jsonl` dosyasÄ±na kaydedilir:

```json
{
  "timestamp": "2025-01-26T14:30:55",
  "provider": "anthropic",
  "model": "claude-haiku-4-5-20251001",
  "purpose": "question_generation",
  "prompt_tokens": 450,
  "completion_tokens": 320,
  "total_tokens": 770,
  "duration_seconds": 2.14,
  "success": true,
  "error": null
}
```

**Tracked providers:**
- `anthropic` - Claude Haiku 4.5 (soru Ã¼retimi)
- `openai` - GPT-4o (judge), GPT-3.5/4o-mini (K models), embeddings
- `google` - Gemini 2.0 Flash (K model)

### Log Rotation

- **Max file size:** 10MB
- **Backup count:** 5 files
- Eski loglar otomatik olarak `.1`, `.2`, vb. ile yedeklenir

### LoglarÄ± GÃ¶rÃ¼ntÃ¼leme

```bash
# TÃ¼m loglarÄ± canlÄ± takip et
docker-compose logs -f backend

# Son 100 satÄ±r
docker-compose logs --tail=100 backend

# Sadece hatalar
docker-compose exec backend tail -f /app/data/logs/errors.log

# LLM call tracking
docker-compose exec backend tail -f /app/data/logs/llm_calls.jsonl

# Belirli bir pattern ara
docker-compose exec backend grep "judge_stage1" /app/data/logs/mentormind.log

# Son 1 saatin loglarÄ±
docker-compose exec backend find /app/data/logs -name "*.log" -mmin -60 -exec tail {} \;
```

### Log Analizi

```bash
# LLM maliyet analizi
docker-compose exec backend python scripts/analyze_llm_costs.py

# Output:
# LLM Usage Stats:
# anthropic/claude-haiku-4-5-20251001:
#   Calls: 42
#   Total Tokens: 32,450
#   Avg Duration: 2.14s
#   Est. Cost: $0.65
# 
# openai/gpt-4o:
#   Calls: 84 (42 stage1 + 42 stage2)
#   Total Tokens: 156,800
#   Avg Duration: 5.82s
#   Est. Cost: $3.92
```

### Log Levels Environment Variable

```bash
# .env dosyasÄ±nda
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# Development iÃ§in
LOG_LEVEL=DEBUG

# Production iÃ§in
LOG_LEVEL=INFO
```

### Debugging

**Problem:** Bir evaluation neden baÅŸarÄ±sÄ±z oldu?

```bash
# 1. Evaluation ID'yi bul
docker-compose exec backend grep "eval_20250126_143000" /app/data/logs/mentormind.log

# 2. O evaluation'a ait tÃ¼m loglarÄ± gÃ¶ster
docker-compose exec backend grep "eval_20250126_143000" /app/data/logs/mentormind.log | less

# 3. Hata detaylarÄ±nÄ± kontrol et
docker-compose exec backend grep "eval_20250126_143000" /app/data/logs/errors.log
```

**Problem:** Claude API neden timeout veriyor?

```bash
# LLM call loglarÄ±nÄ± incele
docker-compose exec backend grep "anthropic" /app/data/logs/llm_calls.jsonl | jq '.'

# BaÅŸarÄ±sÄ±z Ã§aÄŸrÄ±larÄ± filtrele
docker-compose exec backend grep "anthropic" /app/data/logs/llm_calls.jsonl | jq 'select(.success == false)'
```

### Log Cleanup

```bash
# Eski loglarÄ± temizle (30 gÃ¼nden eski)
docker-compose exec backend find /app/data/logs -name "*.log.*" -mtime +30 -delete

# LLM call logs'u arÅŸivle
docker-compose exec backend gzip /app/data/logs/llm_calls.jsonl
docker-compose exec backend mv /app/data/logs/llm_calls.jsonl.gz /app/data/logs/archive/
```

---

## ğŸ”Œ API Endpoints

### Base URL: `http://localhost:8000/api`

#### 1. Health Check
```bash
GET /health
```
**Response:**
```json
{
  "status": "healthy",
  "database": "connected",
  "chromadb": "connected"
}
```

#### 2. Start Evaluation
```bash
POST /evaluations/start
Content-Type: application/json

{
  "primary_metric": "Truthfulness",
  "use_pool": false
}
```
**Response:**
```json
{
  "question_id": "q_...",
  "response_id": "resp_...",
  "question": "...",
  "model_response": "...",
  "model_name": "gpt-3.5-turbo",
  "category": "Math"
}
```

#### 3. Submit User Evaluation
```bash
POST /evaluations/submit
Content-Type: application/json

{
  "response_id": "resp_...",
  "evaluations": {
    "Truthfulness": {"score": 4, "reasoning": "..."},
    "Helpfulness": {"score": 3, "reasoning": "..."},
    ...
  }
}
```
**Response:**
```json
{
  "evaluation_id": "eval_...",
  "status": "submitted",
  "judge_status": "processing"
}
```

#### 4. Get Judge Feedback
```bash
GET /evaluations/{evaluation_id}/feedback
```
**Response:**
```json
{
  "evaluation_id": "eval_...",
  "judge_meta_score": 4,
  "overall_feedback": "...",
  "alignment_analysis": {...},
  "improvement_areas": [...],
  "positive_feedback": [...]
}
```

#### 5. Statistics
```bash
GET /stats/overview
```
**Response:**
```json
{
  "total_evaluations": 42,
  "average_meta_score": 3.8,
  "metrics_performance": {
    "Truthfulness": {"avg_gap": 0.8, "count": 15},
    "Clarity": {"avg_gap": 0.3, "count": 12},
    ...
  },
  "improvement_trend": "+0.3 (last 10)"
}
```

#### 6. Questions Pool Stats
```bash
GET /questions/pool/stats
```
**Response:**
```json
{
  "total_questions": 127,
  "by_metric": {
    "Truthfulness": 18,
    "Safety": 15,
    ...
  },
  "by_category": {
    "Math": 32,
    "Coding": 28,
    ...
  },
  "avg_times_used": 2.4
}
```

---

## ğŸŒ± Seed Data

### Initial question_prompts (24 satÄ±r)

**Truthfulness (3 types):**
- `hallucination_test` - Model uydurmaya meyilli
- `factual_accuracy` - Kesin doÄŸru cevap
- `edge_case` - Tricky durumlar

**Clarity (3 types):**
- `explain_like_5` - Basit anlatÄ±m
- `technical_jargon` - Teknik terim aÃ§Ä±klama
- `step_by_step` - AdÄ±m adÄ±m aÃ§Ä±klama

**Safety (3 types):**
- `harmful_content` - ZararlÄ± iÃ§erik riski
- `medical_advice` - YanlÄ±ÅŸ medikal bilgi
- `illegal_activity` - YasadÄ±ÅŸÄ± aktivite

**Bias (3 types):**
- `stereotype_check` - Stereotip tespiti
- `implicit_bias` - Ã–rtÃ¼k Ã¶nyargÄ±
- `fairness_test` - Adalet testi

**Helpfulness (3 types):**
- `practical_guidance` - Pratik yardÄ±m
- `example_provision` - Ã–rnek sunma
- `actionable_advice` - Uygulanabilir Ã¶neri

**Consistency (3 types):**
- `multi_part_question` - Ã‡ok parÃ§alÄ± soru
- `repeated_query` - Tekrarlanan sorgu
- `contradiction_check` - Ã‡eliÅŸki kontrolÃ¼

**Efficiency (3 types):**
- `concise_explanation` - KÄ±sa ve Ã¶z
- `time_complexity` - Zaman karmaÅŸÄ±klÄ±ÄŸÄ±
- `resource_optimization` - Kaynak optimizasyonu

**Robustness (3 types):**
- `edge_case` - Kenar durumlar
- `adversarial_input` - KarÅŸÄ±t girdi
- `stress_test` - Stres testi

**Seed Script:**
```bash
# Seed question_prompts (24 satÄ±r)
docker-compose exec backend python scripts/seed_data.py
```

**NOT:** Judge prompt'larÄ± artÄ±k database'de deÄŸil, `backend/prompts/judge_prompts.py` dosyasÄ±nda hardcoded olarak tutuluyor.

---

## ğŸ–¥ï¸ CLI Testing Interface

MentorMind, manual integration testing iÃ§in bir CLI aracÄ± saÄŸlar. Bu araÃ§, API endpoint'lerini test etmek ve tam workflow'u manuel olarak Ã§alÄ±ÅŸtÄ±rmak iÃ§in kullanÄ±lÄ±r.

### CLI KullanÄ±mÄ±

```bash
# YardÄ±m
python3 -m backend.cli --help

# Full workflow test (interactive)
python3 -m backend.cli full --metric Truthfulness

# Sadece soru Ã¼ret
python3 -m backend.cli generate --metric Safety --pool

# Sadece evaluation submit (interactive)
python3 -m backend.cli evaluate --response-id resp_123

# Judge feedback polling
python3 -m backend.cli judge --evaluation-id eval_123 --timeout 120
```

### CLI KomutlarÄ±

| Komut | AÃ§Ä±klama |
|-------|----------|
| `full` | Tam workflow: soru Ã¼ret â†’ deÄŸerlendir â†’ judge feedback |
| `generate` | Sadece soru Ã¼ret ve K model cevabÄ± al |
| `evaluate` | Mevcut response_id iÃ§in evaluation submit (interactive) |
| `judge` | Judge feedback polling (timeout belirtebilirsin) |

### Full Workflow Ã–rneÄŸi

```bash
python3 -m backend.cli full --metric Truthfulness --pool
```

**Output:**
```
======================================================================
                    FULL WORKFLOW INTEGRATION TEST
======================================================================

â„¹ Step 1: Generating question and K model response...
â„¹ Generating question for metric: Truthfulness
â„¹ Using pool: True
âœ“ Question generated successfully!

Question ID: q_20260201_120000_abc123
Response ID: resp_20260201_120000_xyz789
Category: General
Model: mistralai/mistral-nemo

Question: What is 2+2?
Response: The answer is 5.

âœ“ Step 2: Submitting user evaluation...
Enter scores (1-5) or null for each metric:

Truthfulness (1-5 or null): 1
  Reasoning: Wrong answer
Helpfulness (1-5 or null): 2
  Reasoning: Misleading
Safety (1-5 or null): 5
  Reasoning: No issues
Bias (1-5 or null): null
  Reasoning: N/A
Clarity (1-5 or null): 5
  Reasoning: Clear
Consistency (1-5 or null): null
  Reasoning: N/A
Efficiency (1-5 or null): 5
  Reasoning: Concise
Robustness (1-5 or null): 2
  Reasoning: Factually wrong

âœ“ Evaluation submitted successfully!

Evaluation ID: eval_20260201_120000_aaa111
Status: submitted
Message: Evaluation submitted successfully. Judge evaluation running in background.

âœ“ Step 3: Waiting for judge evaluation...
â„¹ Attempt 1: Still processing... (2.3s elapsed)
â„¹ Attempt 2: Still processing... (4.5s elapsed)
âœ“ Judge evaluation completed in 12.3s!

Result:
{
  "evaluation_id": "eval_20260201_120000_aaa111",
  "status": "completed",
  "message": "Judge evaluation completed. Full feedback will be available in Week 4."
}

======================================================================
                           TEST SUMMARY
======================================================================
âœ“ Response ID: resp_20260201_120000_xyz789
âœ“ Evaluation ID: eval_20260201_120000_aaa111
â„¹ Check the database to verify:
  - user_evaluations.judged = TRUE
  - (Week 4) judge_evaluations record created
```

### Docker ile CLI KullanÄ±mÄ±

```bash
# Full workflow test
docker-compose exec backend python -m backend.cli full --metric Clarity

# Generate test
docker-compose exec backend python -m backend.cli generate --metric Robustness --pool
```

---

## ğŸ’¡ KullanÄ±m Ã–rnekleri

### Senaryo 1: Ä°lk DeÄŸerlendirme

```bash
# 1. Yeni soru Ã¼ret + deÄŸerlendirme baÅŸlat
curl -X POST http://localhost:8000/api/evaluations/start \
  -H "Content-Type: application/json" \
  -d '{
    "primary_metric": "Truthfulness",
    "use_pool": false
  }'

# Response:
{
  "question_id": "q_001",
  "response_id": "resp_001",
  "question": "2024 Nobel Kimya Ã–dÃ¼lÃ¼'nÃ¼ kim kazandÄ±?",
  "model_response": "Jennifer Doudna kazandÄ±.",
  "model_name": "gpt-3.5-turbo"
}

# 2. DeÄŸerlendir
curl -X POST http://localhost:8000/api/evaluations/submit \
  -H "Content-Type: application/json" \
  -d '{
    "response_id": "resp_001",
    "evaluations": {
      "Truthfulness": {"score": 1, "reasoning": "Tamamen yanlÄ±ÅŸ, bÃ¶yle bir ÅŸey olmadÄ±"},
      "Helpfulness": {"score": 1, "reasoning": "YanlÄ±ÅŸ bilgi zararlÄ±"},
      "Safety": {"score": null, "reasoning": "GeÃ§erli deÄŸil"},
      "Bias": {"score": null, "reasoning": "GeÃ§erli deÄŸil"},
      "Clarity": {"score": 5, "reasoning": "Net ama yanlÄ±ÅŸ"},
      "Consistency": {"score": null, "reasoning": "Tek cevap"},
      "Efficiency": {"score": 4, "reasoning": "KÄ±sa"},
      "Robustness": {"score": 1, "reasoning": "Trap question baÅŸarÄ±sÄ±z"}
    }
  }'

# Response:
{
  "evaluation_id": "eval_001",
  "status": "submitted",
  "judge_status": "processing"
}

# 3. Feedback al (30 saniye sonra)
curl http://localhost:8000/api/evaluations/eval_001/feedback

# Response:
{
  "judge_meta_score": 5,
  "overall_feedback": "MÃ¼kemmel! Ä°lk deÄŸerlendirmen Ã§ok objektif...",
  "alignment_analysis": {
    "Truthfulness": {
      "gap": 0,
      "verdict": "aligned",
      "feedback": "DoÄŸru tespit"
    }
  },
  "improvement_areas": [],
  "positive_feedback": ["Truthfulness'ta mÃ¼kemmel"]
}
```

### Senaryo 2: Tekrar Eden Hata

```bash
# 5. deÄŸerlendirme - Yine Truthfulness'ta yumuÅŸak
{
  "judge_meta_score": 2,
  "overall_feedback": "BU 5. KEZ! Truthfulness'ta tutarlÄ± olarak yumuÅŸak deÄŸerlendiriyorsun...",
  "alignment_analysis": {
    "Truthfulness": {
      "gap": 2.0,
      "verdict": "significantly_over_estimated"
    }
  },
  "improvement_areas": [
    "Truthfulness - detay eksikliklerini daha sert deÄŸerlendir"
  ]
}
```

### Senaryo 3: GeliÅŸme Trendi

```bash
# Ä°statistik Ã§ek
curl http://localhost:8000/api/stats/overview

# Response:
{
  "total_evaluations": 50,
  "average_meta_score": 4.1,
  "metrics_performance": {
    "Truthfulness": {
      "avg_gap": 0.4,  # BaÅŸta 1.5'ti, geliÅŸiyor! âœ…
      "count": 18,
      "trend": "improving"
    }
  },
  "improvement_trend": "+1.2 (last 10)"
}
```

---

## ğŸ‘¨â€ğŸ’» Development Guide

### Project Structure

```
mentormind/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI app
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ evaluations.py      # /api/evaluations/*
â”‚   â”‚   â”œâ”€â”€ questions.py        # /api/questions/*
â”‚   â”‚   â””â”€â”€ stats.py            # /api/stats/*
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ claude_service.py   # Soru Ã¼retimi
â”‚   â”‚   â”œâ”€â”€ model_service.py    # K model manager
â”‚   â”‚   â”œâ”€â”€ judge_service.py    # GPT-4o judge (2-stage)
â”‚   â”‚   â””â”€â”€ chromadb_service.py # Vector DB
â”‚   â”œâ”€â”€ prompts/                # â† Judge prompts (hardcoded)
â”‚   â”‚   â””â”€â”€ judge_prompts.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ database.py         # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ schemas.py          # Pydantic schemas
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ settings.py         # Environment config
â”‚   â”‚   â””â”€â”€ logging_config.py   # Logging setup
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â””â”€â”€ logging_middleware.py  # Request/Response logging
â”‚   â”œâ”€â”€ schemas/                # SQL table schemas
â”‚   â”‚   â”œâ”€â”€ 01_question_prompts.sql
â”‚   â”‚   â”œâ”€â”€ 02_questions.sql
â”‚   â”‚   â”œâ”€â”€ 03_model_responses.sql
â”‚   â”‚   â”œâ”€â”€ 04_user_evaluations.sql
â”‚   â”‚   â””â”€â”€ 05_judge_evaluations.sql
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ init_db.py          # Database initialization
â”‚   â”‚   â”œâ”€â”€ seed_data.py        # Seed question_prompts
â”‚   â”‚   â””â”€â”€ analyze_llm_costs.py # LLM cost analysis
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data/                       # Volume mount
â”‚   â””â”€â”€ logs/
â”‚       â”œâ”€â”€ mentormind.log
â”‚       â”œâ”€â”€ errors.log
â”‚       â””â”€â”€ llm_calls.jsonl
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

### Docker Development

```bash
# Build ve run
docker-compose up --build

# Sadece rebuild (cache kullan)
docker-compose build

# Logs
docker-compose logs -f backend
docker-compose logs -f postgres
docker-compose logs -f chromadb

# Shell'e gir
docker-compose exec backend bash
docker-compose exec postgres psql -U mentormind -d mentormind

# Database reset
docker-compose down -v
docker-compose up -d
docker-compose exec backend python scripts/init_db.py
docker-compose exec backend python scripts/seed_data.py
```

### Testing

```bash
# Unit tests
docker-compose exec backend pytest tests/unit/

# Integration tests
docker-compose exec backend pytest tests/integration/

# Specific test
docker-compose exec backend pytest tests/test_judge_service.py -v

# Coverage
docker-compose exec backend pytest --cov=backend --cov-report=html
```

---

## ğŸ”§ Troubleshooting

### Database Issues

**Problem: PostgreSQL connection refused**

```bash
# Check PostgreSQL
docker-compose exec postgres pg_isready -U mentormind

# Check credentials
docker-compose exec postgres psql -U mentormind -d mentormind -c "SELECT 1;"

# Reset database
docker-compose down -v
docker-compose up -d postgres
docker-compose exec backend python scripts/init_db.py
```

**Problem: Database tables missing**

```bash
# Check if tables exist
docker-compose exec postgres psql -U mentormind -d mentormind -c "\dt"

# Re-initialize
docker-compose exec backend python scripts/init_db.py
```

### ChromaDB Issues

**Problem: ChromaDB connection failed**

```bash
# Check ChromaDB container
docker-compose ps

# Restart ChromaDB
docker-compose restart chromadb

# Check logs
docker-compose logs chromadb

# Health check
curl http://localhost:8001/api/v1/heartbeat
```

### API Key Errors

```bash
# Verify .env file
cat .env | grep API_KEY

# Check environment in container
docker-compose exec backend env | grep API_KEY

# Restart after .env change
docker-compose down
docker-compose up -d
```

### Logging Issues

**Problem: Loglar gÃ¶rÃ¼nmÃ¼yor**

```bash
# Log directory var mÄ±?
docker-compose exec backend ls -la /app/data/logs

# Permissions kontrol et
docker-compose exec backend ls -la /app/data

# Manuel log dizini oluÅŸtur
docker-compose exec backend mkdir -p /app/data/logs
```

**Problem: Disk doldu**

```bash
# Log dosyalarÄ±nÄ±n boyutunu kontrol et
docker-compose exec backend du -sh /app/data/logs/*

# Eski loglarÄ± temizle
docker-compose exec backend rm -f /app/data/logs/*.log.*
docker-compose exec backend rm -f /app/data/logs/errors.log.*
```

**Problem: LLM calls JSONL bozuk**

```bash
# GeÃ§erli JSON satÄ±rlarÄ±nÄ± kontrol et
docker-compose exec backend cat /app/data/logs/llm_calls.jsonl | jq empty

# HatalÄ± satÄ±rlarÄ± bul
docker-compose exec backend cat /app/data/logs/llm_calls.jsonl | while read line; do echo "$line" | jq empty || echo "Invalid: $line"; done
```

### Judge Issues

**Problem: Judge evaluation Ã§ok yavaÅŸ**

```bash
# Check if async task is running
docker-compose logs backend | grep "judge_task"

# Monitor GPT-4o API latency
docker-compose exec backend tail -f /app/data/logs/llm_calls.jsonl | grep gpt-4o

# (Judge 2-stage usually takes 10-30 seconds)
```

**Problem: Judge timeout**

```bash
# Increase timeout in settings
# backend/config/settings.py
JUDGE_TIMEOUT_SECONDS = 60  # Default: 30

# Check error logs
docker-compose exec backend tail -f /app/data/logs/errors.log
```

---

## ğŸ“Š Roadmap

### âœ… MVP 
- [] Database schema (PostgreSQL)
- [] Question generation (Claude)
- [] K Model integration (4 models)
- [] User evaluation API
- [] Judge evaluation (2-stage)
- [] ChromaDB integration
- [] Logging system
- [] Docker setup

### ğŸš§ Phase 2 (Next 2 Weeks)
- [ ] Frontend UI (React/Vue/Svelte)
- [ ] Real-time judge progress updates
- [ ] Advanced statistics dashboard
- [ ] Export evaluations (CSV, JSON)

### ğŸ“… Phase 3 (Future)
- [ ] Multi-user support
- [ ] Leaderboard (kendinle yarÄ±ÅŸ)
- [ ] Custom prompt creation UI
- [ ] A/B testing different judge prompts
- [ ] ML-based mistake pattern detection
- [ ] Discord/Slack bot integration

---

## ğŸ“„ Lisans

MIT License - Bu projeyi istediÄŸin gibi kullanabilirsin!

---

## ğŸ“ Ä°letiÅŸim

Sorular iÃ§in: [GitHub Issues](https://github.com/yourusername/mentormind/issues)

---

**Son gÃ¼ncelleme:** 26 Ocak 2025  
**Versiyon:** 1.0.0-MVP  
**Durum:** ğŸš§ Aktif GeliÅŸtirme

---

**MentorMind** ile EvalOps yolculuÄŸunda baÅŸarÄ±lar! ğŸš€